[
  {
    "objectID": "posts/Revelando los Sentimientos Guatemaltecos/index.html",
    "href": "posts/Revelando los Sentimientos Guatemaltecos/index.html",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "",
    "text": "Con las elecciones presidenciales a menos de un mes de distancia, la importancia de comprender la voz del p√∫blico no puede subestimarse. A trav√©s del uso de t√©cnicas avanzadas, esta entrada busca no solo descubrir patrones en el sentimiento p√∫blico, sino tambi√©n demostrar c√≥mo la nueva generaci√≥n de modelos de lenguaje pueden emplearse para ofrecer perspectivas √∫nicas y valiosas en la din√°mica social y pol√≠tica de los pa√≠ses. Espec√≠ficamente, utilizaremos dos poderosos recursos para llevar a cabo este an√°lisis: la API de Twitter y la API de OpenAI.\n\nPrimero, descargaremos tweets que mencionan a los cinco candidatos que lideraban la encuesta de Prensa Libre publicada a principios de mayo desde la Twitter API V2 utilizando Python.\nEstos tweets, entonces, servir√°n como nuestro conjunto de datos base para extraer caracter√≠sticas utilizando la API de OpenAI y su modelo de lenguaje GPT-3.5-turbo, empleando una t√©cnica llamada zero-shot feature extraction.\nPor √∫ltimo, terminaremos realizando una breve exploraci√≥n de los resultados utilizando R, mi herramienta favorita cuando se trata de procedimientos estad√≠sticos y de an√°lisis de datos.\n\nA lo largo del blog, explicaremos c√≥mo funcionan estas APIs, c√≥mo utilizarlas implementando clases en Python lo suficientemente robustas como para lidiar con sus errores y excepciones, y c√≥mo estos resultados se pueden utilizar para realizar un an√°lisis profundo y significativo.\nDesde hace varios a√±os, la intersecci√≥n entre las ciencias de la computaci√≥n y las ciencias sociales se ha transformado en uno de mis temas favoritos. Es verdaderamente emocionante darse cuenta de que nos encontramos en un momento √∫nico, en el cual es posible emplear tecnolog√≠as de vanguardia para obtener insights de eventos significativos. Quiz√°s es m√°s emocionante a√∫n considerar que este tipo de an√°lisis habr√≠a sido pr√°cticamente imposible de llevar a cabo hace solo cinco a√±os. ¬øLista? ¬øListo?"
  },
  {
    "objectID": "posts/Revelando los Sentimientos Guatemaltecos/index.html#extracci√≥n-de-tweets-con-tweepy",
    "href": "posts/Revelando los Sentimientos Guatemaltecos/index.html#extracci√≥n-de-tweets-con-tweepy",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "2 Extracci√≥n de tweets con Tweepy",
    "text": "2 Extracci√≥n de tweets con Tweepy\nOkay. Cartas sobre la mesa. Implementar estos programas ha conllevado bastante de mi tiempo libre en las √∫ltimas semanas; estoy seguro de que leer todos los detalles sobre estas implementaciones tambi√©n requerir√≠a de una cantidad de tiempo considerable, por lo que en esta y la siguiente secci√≥n √∫nicamente haremos un repaso por los puntos m√°s importantes1.1¬†Para los lectores m√°s interesados he publicado este repositorio en el es posible encontrar todos los programas y m√≥dulos utilizados para generar estos resultados. Las clases y sus m√©todos fueron debidamente documentados, pero si√©ntete libre de contactarme si te gustar√≠a saber m√°s sobre los detalles de estas implementaciones.\n\nEnviando requests hacia la API\nLa estructura del repositorio es extremadamente sencilla y el dise√±o de las clases se adhiere al single-responsibility principle. El directorio modules contiene las clases encargadas de comunicarse con las APIs, mientras que los programas download_tweets.py y extract_features.py se encargan de descargar la informaci√≥n en batches. En el caso del procedimiento de extracci√≥n de tweets, el m√≥dulo twitter_request.py contiene a la clase TwitterRequest, responsable de comunicarse con la Twitter API V2 siguiendo los siguientes pasos:\n\nConsultar el endpoint GET_2_tweets_search_recent de la API utilizando al m√©todo search_recent_tweets para extraer los tweets que necesitamos.\nProcesar la respuesta de la API en formato JSON, convierti√©ndola primero en un DataFrame.\nSeparar a ese DataFrame en dos: Uno que contenga informaci√≥n de los tweets y otro que contenga informaci√≥n de los usuarios que realizaron dichas publicaciones.\nPreprocesar ambos DataFrames para estandarizar los nombres de las columnas y cambiar las columnas que contienen fechas a un formato m√°s conveniente.\n\nEn esta clase, el m√©todo m√°s importante es make_request. Este m√©todo est√° decorado con @backoff.on_exception una forma incre√≠blemente conveniente de hacer exponential backoff en el momento en el que se produzca un error por generar demasiados requests o una excepci√≥n producto de un timeout entre nuestro cliente y el servidor. En cualquiera de estos casos, la funci√≥n esperar√° un tiempo exponencialmente creciente antes de volver a intentarlo y √∫nicamente har√° un m√°ximo de 5 intentos.\n\nimport tweepy\nimport backoff\nimport requests\nfrom modules import setup_logger\n\n@backoff.on_exception(\n        backoff.expo, \n        (tweepy.errors.TooManyRequests, requests.exceptions.ReadTimeout),\n        max_tries=5\n    )\ndef make_request(self) -> \"TwitterRequest\":\n    self.query = f\"{self.query} -is:retweet -is:reply\"\n    self.logger.info(\"Making request with query: %s\", self.query)\n\n    try:\n        self.tweets = client.search_recent_tweets(\n            query = self.query,\n            start_time = self.start_time,\n            end_time = self.end_time,\n            max_results = self.max_results,\n            tweet_fields = [\n                \"id\", \"author_id\", \"created_at\", \"text\", \n                \"public_metrics\", \"possibly_sensitive\", \"lang\"\n            ],\n            user_fields = [\n                \"id\", \"username\", \"name\", \"location\", \"created_at\", \"description\", \n                \"profile_image_url\", \"verified\", \"public_metrics\"\n            ],\n            expansions = [\n                \"author_id\", \"referenced_tweets.id\"\n            ]\n        )\n        \n        if self.tweets is None or self.tweets.data is None:\n            self.logger.error(\"No tweets returned from request\")\n            return self\n          \n    except Exception as e: \n        self.logger.error(\"Exception occurred\", exc_info=True)\n        raise \n\n    self.logger.info(\"Request completed successfully.\")\n    return self\n\nEl mismo m√©todo define el query que se llevar√° a cabo, que en nuestro caso es un string con el nombre del candidato mencionado en los tweets, pero instruimos a la API para que no devuelva tweets que sean retweets o respuestas (f\"{self.query} -is:retweet -is:reply\"). En la solicitud, se incluyen campos espec√≠ficos de los tweets y del usuario que los public√≥. Por ejemplo, del tweet se solicita el ID, el autor, la fecha de creaci√≥n, el texto, las m√©tricas como cantidad de retweets, likes o respuestas, as√≠ como si la publicaci√≥n tienen contenido sensible y cu√°l es su idioma. Del usuario solicitamos el ID, el nombre de usuario, el nombre, la ubicaci√≥n, la fecha de creaci√≥n del perfil, la descripci√≥n, la imagen de perfil, sus m√©tricas p√∫blicas y si su perfil est√° verificado o no2.2¬†Aunque mis procedimientos extrajeron toda esta informaci√≥n para m√°s de 5,000 tweets que fueron publicados por m√∫ltiples usuarios durante las √∫ltimas dos semanas, el Developer Agreement and Policy de Twitter me prohibe publicar la totalidad de la informaci√≥n. Sin embargo, he publicado un dataset reducido que contiene 1,420 tweets con sus respectivas m√©tricas p√∫blicas y las caracter√≠sticas extra√≠das utilizando al modelo de lenguaje.\nSi la solicitud es exitosa y se obtiene cierta cantidad de tweets que son almacenados en el atributo self.tweets. Si no se obtiene ning√∫n tweet, se registra un error en el log. Si ocurre alguna otra excepci√≥n durante la solicitud, tambi√©n se registra en el log y es levantada para que pueda ser manejada por el decorador encargado de hacer exponential backoff. Por √∫ltimo, si la solicitud se complet√≥ con √©xito, registramos este hecho en el log y retornamos al objeto self, para darnos la posibilidad de hacer method chaining.\nDe esta forma, utilizar la clase TwitterRequest requiere √∫nicamente de inicializarla con los par√°metros query, start_time, end_time y max_results. Like so:\n\nfrom datetime import datetime\nfrom modules import TwitterRequest\n\ntweets, users = (\n  TwitterRequest(\n      query='zury rios',\n      start_time=datetime(2023, 5, 20, 15, 00),\n      end_time=datetime(2023, 5, 21, 15, 00),\n      max_results=10\n  )\n  .make_request()\n  .tweets_to_dataframe()\n  .users_to_dataframe()\n  .segregate_dataframe()\n  .preprocess_data(\n      tweets_prefix='tw_',\n      users_prefix='us_'\n  )\n)\n\n\n\nDescargando tweets en batches\nTenemos una forma de comunicarnos con la API de Twitter, pedirle los datos que nos interesan y preprocesarlos para que est√©n listos para el resto del pipeline. Sin embargo, Twitter no nos va a hacer la vida f√°cil. Su API √∫nicamente permite descargar tweets publicados en los √∫ltimos 7 d√≠as en batches de 60 tweets como m√°ximo en un espacio de 15 minutos.33¬†Overprotective much?\nNi modo. El programa download_tweets.py esta dise√±ado para lidiar con las restricciones de la API de Twitter. El m√©todo download_tweets es un componente de la clase DownloadTweets. En resumen, este m√©todo descarga tweets y usuarios (utilizando el m√©todo get_batch, que es un wrapper para la clase TwitterRequest). Para cada candidato y rango de fechas, se invoca el m√©todo get_batch para obtener los tweets y usuarios, que se recopilan y se concatenan en dos atributos: self.tweets y self.users. Finalmente, el m√©todo devuelve estos dos DataFrames que contienen todos los tweets y usuarios recolectados.\n\ndef download_tweets(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    self.generate_dates()\n    logger.info(f\"Generated {len(self.dates)} date pairs for tweet downloads.\")\n\n    # Collect tweets and users for each candidate.\n    tweets_collector, users_collector = [], []\n    for candidate in self.candidates:\n\n        # Collect tweets and users for each date.\n        dates_tweets_collector, dates_users_collector = [], []\n        for start_date, end_date in self.dates:\n\n            tweets, users = self.get_batch(candidate, start_date, end_date)\n            dates_tweets_collector.append(tweets)\n            dates_users_collector.append(users)\n\n        tweets_collector.append(pd.concat(dates_tweets_collector))\n        users_collector.append(pd.concat(dates_users_collector))\n\n    self.tweets = pd.concat(tweets_collector, axis=0, ignore_index=True)\n    self.users = pd.concat(users_collector, axis=0, ignore_index=True)\n\n    logger.info(\n      f\"Downloaded a total of {len(self.tweets)} tweets and {len(self.users)} users.\"\n    )\n    return self.tweets, self.users"
  },
  {
    "objectID": "posts/Revelando los Sentimientos Guatemaltecos/index.html#zero-shot-feature-extraccion-con-gpt-3.5",
    "href": "posts/Revelando los Sentimientos Guatemaltecos/index.html#zero-shot-feature-extraccion-con-gpt-3.5",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "3 Zero-shot feature extraccion con GPT-3.5",
    "text": "3 Zero-shot feature extraccion con GPT-3.5\nAhora viene una de mis partes favoritas en este proceso. Vamos a usar a GPT-3.5 para generar nuevas features basadas en los tweets que hemos extra√≠do. ¬øC√≥mo? Much in the same way we downloaded tweets, usando una clase llamada OpenAIRequest (que se encargar√° de comunicarse con la API) y otra, llamada FeatureExtraction, que nos servir√° para iterar en las filas del DataFrame que contiene las publicaciones de los usuarios de Twitter.\n\nEnviando requests hacia la API\nEl primer m√©todo relevante en la clase OpenAIRequest es make_request. De manera similar al m√©todo con el mismo nombre en el procedimiento de extracci√≥n de tweets, esta funci√≥n est√°tica est√° decorada con backoff.on_exception para poder lidiar con errores en el caso de un timeout o por sobrepasar los l√≠mites de uso de la API. En el caso del modelo gpt-3.5-turbo, podemos realizar una cantidad de 3,500 requests por minuto o enviar al endpoint un m√°ximo 90,000 tokens en la misma cantidad de tiempo, lo que pase primero.\nComo veremos m√°s adelante, el prompt con el que instruiremos al modelo tiene, en promedio, 600 tokens.4 En teor√≠a, esto quiere decir que (bas√°ndonos en el l√≠mite de TPM) podemos enviar 150 requests cada 60 segundos. En la pr√°ctica, though, los tiempos de inferencia de los modelos de lenguaje son bastante altos. En nuestro caso, el tiempo de procesamiento por cada tweet fue de aproximadamente 15 segundos, as√≠ que extraer caracter√≠sticas para 1,420 tweets fue un proceso que tom√≥ 6 horas, give or take.4¬†OpenAI pone a nuestra disposici√≥n este tokenizer, una herramienta que nos permite hacer un recuento de la cantidad de tokens en nuestros prompts. En lo personal, me sirve much√≠simo para calcular los costos de procesamiento y para asegurarme de que mis prompts sean eficientes en t√©rminos de longitud.\nLos par√°metros que acepta make_request son nuestro prompt, la especificaci√≥n del modelo que queremos utilizar y temperature, un par√°metro del modelo de lenguaje que puede ser un n√∫mero entre 0 y 1. La temperatura controla el grado de aleatoriedad en las respuestas del modelo. Un valor de temperature m√°s alto (cerca de 1) hace que el modelo genere respuestas m√°s diversas y creativas, mientras que un valor m√°s bajo (cerca de 0) hace que las respuestas sean m√°s determin√≠sticas o consistentes.55¬†Los transformers, como GPT, son en s√≠ mismos arquitecturas de redes neuronales determin√≠sticas (es decir, generan la misma salida para una entrada espec√≠fica). Sin embargo, se introduce aleatoriedad durante la generaci√≥n de texto a trav√©s del muestreo de diferentes secuencias de palabras de acuerdo a las probabilidades de salida del modelo. En general, cuando usamos al modelo como un paso de procesamiento dentro de nuestros pipelines, queremos que cada respuesta sea lo m√°s consistente posible para reducir las posibilidades de introducir bugs en nuestro sistema.\n\n@staticmethod\n@backoff.on_exception(\n    backoff.expo, \n    (openai.error.RateLimitError, requests.exceptions.ReadTimeout),\n    max_tries=5\n)\ndef make_request(prompt: str, model: str = \"gpt-3.5-turbo\", temperature: float = 0) -> str: \n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=temperature, \n    )\n    return response.choices[0].message[\"content\"]\n\nEl siguiente m√©todo relevante en la clase OpenAIRequest es extract_features. En este m√©todo, definimos el prompt que ser√° utilizado para instruir al modelo de lenguaje. Este prompt es parametrizado a trav√©s de una f string, para permitirnos cambiar f√°cilmente la entrada seg√∫n sea necesario.\nUna vez definido el prompt, realizamos la consulta a la API de OpenAI utilizando el m√©todo make_request. Si la respuesta obtenida es nula o inv√°lida, registramos un error en el log y retornamos None. Adem√°s, para robustecer al procedimiento y manejar posibles excepciones durante la solicitud a la API, hemos envuelto este segmento de c√≥digo dentro de un bloque Try Except. Si todo sale seg√∫n lo planeado y obtenemos una respuesta v√°lida, la misma es cargada como un diccionario JSON y finalmente retornada por el m√©todo extract_features.\n\ndef extract_features(self, prefix: str) -> dict:\n    prompt = f\"\"\" (...) \"\"\"\n    \n    try:\n        response = OpenAIRequest.make_request(prompt)\n        if response is None:\n            self.logger.error(\"Received invalid response from OpenAI\")\n            return None\n        response = json.loads(response)\n    except Exception as e:\n        self.logger.error(f\"Exception during API request: {e}\")\n        return None\n\n    return response\n\n\n\nPrompt engineering\nTengo que confesar algo. Hace unas semanas le√≠ en una publicaci√≥n de LinkedIn una frase que dec√≠a algo as√≠ como: ‚ÄúCalling ‚Äòprompt engineering‚Äô the action of using ChatGPT today is like calling ‚Äòsearch engineering‚Äô to googling something in the early 2000s‚Äù. Y la confesi√≥n es que‚Ä¶ pienso que es verdad.66¬†Por cierto, ya que estamos confesando cosas. La idea de esta entrada surgi√≥ porque hace poco m√°s de un mes vi los videos del curso de deeplearning.ai llamado ChatGPT Prompt Engineering for Developers. Es un curso extremadamente corto que tiene informaci√≥n √∫til acerca de los casos de uso del modelo desde una perspectiva program√°tica. Si est√°s leyendo este blog, chances are you‚Äôre also gonna like this.\nNontheless, considero que cuando vamos un paso m√°s all√° y utilizamos estos modelos desde los endpoints que OpenAI pone a nuestra disposici√≥n, el hype que existe hacia el t√©rmino est√° un poco m√°s justificado. Principalmente porque, aunque redactamos la instrucci√≥n en lenguaje natural, el procedimiento recuerda mucho a definir una serie de pasos en cualquier lenguaje de programaci√≥n.\nPara nuestros fines, este prompt dio buenos resultados:\n\n\n\n\n\n\n\n\nprompt = f\"\"\"\nEl siguiente es un tweet que menciona a un candidato presidencial dentro de la contienda electoral 2023 en Guatemala. \n\nPor favor, clasif√≠calo de acuerdo a las siguientes categor√≠as:\n\nValencia (sentimiento general): [positivo, negativo, neutro, otro]\nEmoci√≥n (emoci√≥n principal expresada): [felicidad, tristeza, enojo, miedo, sorpresa, disgusto, otro]\nPostura (actitud hacia el tema): [aprobaci√≥n, desaprobaci√≥n, esperanza, desilusi√≥n, indiferencia, confianza, desconfianza, otro]\nTono (forma de expresarse): [agresivo, pasivo, asertivo, esc√©ptico, ir√≥nico, humor√≠stico, informativo, serio, inspirador, otro]\n\nAdem√°s, eval√∫alo utilizando una escala continua con rango de 0 a 1 en las siguientes dimensiones:\n\nAmabilidad (nivel de cortes√≠a): [0.0 - 1.0]\nLegibilidad (facilidad de lectura): [0.0 - 1.0]\nControversialidad (potencial para generar desacuerdo): [0.0 - 1.0]\nInformatividad (cantidad de informaci√≥n relevante y fundamentada): [0.0 - 1.0]\n\nFormatea tu respuesta como un diccionario de Python con las siguientes llaves:\n\n[valencia, emocion, postura, tono, amabilidad, legibilidad, controversialidad, informatividad]\n\nTweet: '''{self.tweet}'''\n\"\"\"\n\n\n\n\n\n\n\nZero-shot feature extraction\nEstamos muy cerca de obtener los resultados que buscamos. Hasta el momento, los m√≥dulos que hemos implementado nos permiten enviar consultas hacia las APIs de Twitter y OpenAI, as√≠ como descargar batches de tweets que mencionan a los cinco candidatos que encabezaban la encuesta de Prensa Libre, publicada a inicios de mayo. Solo nos falta una forma procesar estos tweets para extraer variables que nos permitan analizarlos a una mayor profundidad. Enter extract_features.py, un programa en el que la clase OpenAIRequest es instanciada dentro de la clase FeatureExtraction.\nEn el constructor de la clase se inicializan las rutas a dos archivos: df_path que es la ruta al archivo csv de entrada que contiene los tweets, y results_df_path que es la ruta al archivo csv de salida donde se almacenar√°n las caracter√≠sticas extra√≠das.\nEl m√©todo principal de esta clase es extract_features. Este m√©todo primero carga los tweets del archivo csv de entrada y elimina los duplicados. Luego intenta cargar las caracter√≠sticas ya extra√≠das del archivo csv de salida. Si este archivo no existe, se inicializa un nuevo DataFrame vac√≠o. El m√©todo luego determina qu√© tweets a√∫n no han sido procesados comparando los tweets en los dos DataFrames y seleccionando aquellos que solo est√°n en el DataFrame de entrada.\n\ndef extract_features(self):\n    df = pd.read_csv(self.df_path)\n    df = df.drop_duplicates(subset=['tw_texto'], keep='first')\n\n    try:\n        df_results = pd.read_csv(self.results_df_path)\n        df_results = df_results.drop_duplicates(subset=['tw_texto'], keep='first')\n    except FileNotFoundError:\n        df_results = pd.DataFrame()\n\n    df_to_process = df[~df['tw_texto'].isin(df_results['tw_texto'])]\n    df_to_process = df_to_process.dropna()\n\n    for index, row in df_to_process.iterrows():\n        tweet = row['tw_texto']\n        response = (\n            OpenAIRequest(tweet)\n            .preprocess_text()\n            .extract_features(prefix='tw_')\n        )\n        df_result = pd.DataFrame([response], index=[index])\n        df_results = pd.concat([df_results, df_result])\n        df_results.to_csv(self.results_df_path, index=False)\n\nA continuaci√≥n, se procesa cada tweet que a√∫n no ha sido procesado. Para cada uno de ellos, se realiza una solicitud a la API de OpenAI para realizar el procedimiento de feature extraction utilizando al modelo gpt-3.5-turbo. Las caracter√≠sticas extra√≠das se a√±aden al DataFrame de resultados, junto con el tweet original y un tag para identificar a qu√© candidato se refiere cada publicaci√≥n.\nFinalmente, despu√©s de procesar cada tweet, el DataFrame de resultados se guarda en el archivo csv de salida. Esto se hace despu√©s de cada tweet para evitar la p√©rdida de informaci√≥n en caso de que se produzca un error durante el procesamiento."
  },
  {
    "objectID": "posts/Revelando los Sentimientos Guatemaltecos/index.html#an√°lisis-de-la-opini√≥n-p√∫blica",
    "href": "posts/Revelando los Sentimientos Guatemaltecos/index.html#an√°lisis-de-la-opini√≥n-p√∫blica",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "4 An√°lisis de la opini√≥n p√∫blica",
    "text": "4 An√°lisis de la opini√≥n p√∫blica\nCool. Todos los procedimientos anteriores nos llevan a esta situaci√≥n en la que tenemos un conjunto de datos listo para ser analizado. Como mencion√© en alg√∫n punto de esta entrada, he publicado este dataset en un repositorio de GitHub. By all means, s√≠entete libre de descargarlo y hacer tu propio an√°lisis exploratorio de datos. Si lo haces, me encantar√≠a conocer tus descubrimientos. El dataset luce m√°s o menos as√≠:\n\n\n\n\n \n  \n    fecha \n    tweet \n    candidato \n    retweets \n    replies \n    likes \n    quotes \n    impresiones \n    valencia \n    emocion \n    postura \n    tono \n    amabilidad \n    legibilidad \n    controversialidad \n    informatividad \n    sensitivo \n  \n \n\n  \n    2023-05-15 \n    Sandra Torres es capaz de todo por mantener el poder. Recibi√≥ dinero sucio, se vendi√≥ con Alejandro Giammattei para evitar que cancelaran su partido, incluy√≥ a se√±alados de corrupci√≥n y particip√≥ de una red de captaci√≥n de fondos il√≠citos. https://t.co/4J2pX1BSx6 \n    sandra torres \n    0 \n    1 \n    0 \n    0 \n    335 \n    negativo \n    enojo \n    desaprobaci√≥n \n    agresivo \n    0.2 \n    0.8 \n    0.9 \n    0.9 \n    FALSE \n  \n  \n    2023-05-15 \n    Sandra Torres anda regalando zapatos a cambio de votos üò° esto tiene que parar https://t.co/8OltsAmeiq \n    sandra torres \n    0 \n    0 \n    5 \n    0 \n    420 \n    negativo \n    enojo \n    desaprobaci√≥n \n    agresivo \n    0.2 \n    0.9 \n    0.8 \n    0.8 \n    FALSE \n  \n  \n    2023-05-15 \n    Y en donde est√°n los ladrones como Sandra torres. Baldizon. Los arzu. Portillo. Los r√≠os montt. Los jimmy morales. Los Giammattei y tantos diputados. Que le han robado millones  al pueblo de guatemala. https://t.co/KVngH40GF9 \n    sandra torres \n    0 \n    0 \n    0 \n    0 \n    234 \n    negativo \n    enojo \n    desaprobaci√≥n \n    agresivo \n    0.2 \n    0.8 \n    0.7 \n    0.9 \n    FALSE \n  \n  \n    2023-05-15 \n    #AlertaPopulista üö®\n\n¬øSandra Torres, sabe cu√°les son derechos humanos?\n\nTe presentamos el episodio n√∫mero 5 de nuestra secci√≥n \"El Populista de la Semana\" con @PalmieriWaelti\n\n#EleccionesGuatemala #Elecciones2023 #Guatemala #EleccionesGT #Populistas #Facts #Noticias https://t.co/MeoijdOE12 \n    sandra torres \n    6 \n    9 \n    17 \n    1 \n    3186 \n    negativo \n    enojo \n    desaprobaci√≥n \n    agresivo \n    0.2 \n    0.9 \n    0.8 \n    0.8 \n    FALSE \n  \n  \n    2023-05-15 \n    La CC dej√≥ en firme la inscripci√≥n del binomio presidencial de la UNE conformado por Sandra Torres y Romeo Estuardo Guerra Lemus. üîΩ https://t.co/yG7AEiXV1Y \n    sandra torres \n    0 \n    0 \n    0 \n    0 \n    129 \n    neutro \n    otro \n    informaci√≥n \n    informativo \n    0.8 \n    1.0 \n    0.2 \n    1.0 \n    FALSE \n  \n  \n    2023-05-15 \n    ü§îü§£\n¬øLa hija de Sandra Torres hablando de izquierdosos?\n¬øWTF?!!! https://t.co/Jd4ZaiQ9K3 https://t.co/qk4vZo50aD \n    sandra torres \n    0 \n    0 \n    2 \n    0 \n    180 \n    negativo \n    enojo \n    desaprobaci√≥n \n    ir√≥nico \n    0.2 \n    1.0 \n    0.8 \n    0.6 \n    TRUE"
  },
  {
    "objectID": "posts/Revelando los Sentimientos Guatemaltecos/index.html#conclusiones",
    "href": "posts/Revelando los Sentimientos Guatemaltecos/index.html#conclusiones",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "5 Conclusiones",
    "text": "5 Conclusiones\n\nLos tiempos de inferencia son altos, as√≠ que los l√≠mites de la API de OpenAI son bastante bondadosos."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science y machine learning",
    "section": "",
    "text": "Revelando los sentimientos guatemaltecos\n\n\nExtracci√≥n zero-shot de variables para tweets acerca de las elecciones presidenciales 2023 usando GPT-3.5\n\n\n\n\nNatural language processing\n\n\nObject-oriented programming\n\n\nData engineering\n\n\n\n\n\n\n\n\n\n\n\n12 jun 2023\n\n\n26 minutos\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "posts/election-insights/index.html",
    "href": "posts/election-insights/index.html",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "",
    "text": "Con las elecciones presidenciales a poco m√°s de dos semanas de distancia, la importancia de comprender la voz del p√∫blico no puede subestimarse. A trav√©s del uso de t√©cnicas avanzadas, esta entrada busca no solo descubrir patrones en el sentimiento p√∫blico, sino tambi√©n demostrar c√≥mo la nueva generaci√≥n de modelos de lenguaje pueden emplearse para ofrecer perspectivas √∫nicas y valiosas en la din√°mica social y pol√≠tica de los pa√≠ses. Espec√≠ficamente, utilizaremos dos poderosos recursos para llevar a cabo este an√°lisis: la API de Twitter y la API de OpenAI.\n\nPrimero, descargaremos tweets que mencionan a los cinco candidatos que lideraban la encuesta de Prensa Libre publicada a principios de mayo desde la Twitter API V2 utilizando Python.\nEstos tweets, entonces, servir√°n como nuestro conjunto de datos base para extraer caracter√≠sticas utilizando la API de OpenAI y su modelo de lenguaje gpt-3.5-turbo, empleando una t√©cnica llamada zero-shot feature extraction.\nPor √∫ltimo, realizaremos una breve exploraci√≥n de los resultados utilizando R, mi herramienta favorita cuando se trata de procedimientos estad√≠sticos y de an√°lisis de datos.\n\nA lo largo del blog, explicaremos c√≥mo funcionan estas APIs, c√≥mo utilizarlas implementando clases en Python lo suficientemente robustas como para lidiar con sus errores y excepciones, y c√≥mo estos resultados se pueden utilizar para realizar un an√°lisis profundo y significativo.\nDesde hace varios a√±os, la intersecci√≥n entre las ciencias de la computaci√≥n y las ciencias sociales se ha transformado en uno de mis temas favoritos. Es verdaderamente emocionante darse cuenta de que nos encontramos en un momento √∫nico, en el cual es posible emplear tecnolog√≠as de vanguardia para obtener insights de eventos significativos. Quiz√°s es m√°s emocionante a√∫n considerar que este tipo de an√°lisis habr√≠a sido pr√°cticamente imposible de llevar a cabo hace solo dos a√±os. ¬øLista? ¬øListo?"
  },
  {
    "objectID": "posts/election-insights/index.html#extracci√≥n-de-tweets-con-tweepy",
    "href": "posts/election-insights/index.html#extracci√≥n-de-tweets-con-tweepy",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "2 Extracci√≥n de tweets con Tweepy",
    "text": "2 Extracci√≥n de tweets con Tweepy\nOkay. Cartas sobre la mesa. Implementar estos programas ha conllevado bastante de mi tiempo libre en las √∫ltimas semanas; estoy seguro de que leer todos los detalles sobre estas implementaciones tambi√©n requerir√≠a de una cantidad de tiempo considerable, por lo que en esta y la siguiente secci√≥n √∫nicamente haremos un repaso por los puntos m√°s importantes1.1¬†Para los lectores m√°s interesados he publicado este repositorio en el es posible encontrar todos los programas y m√≥dulos utilizados para generar estos resultados. Las clases y sus m√©todos fueron debidamente documentados, pero si√©ntete libre de contactarme si te gustar√≠a saber m√°s sobre los detalles de estas implementaciones.\n\nEnviando requests hacia la API\nLa estructura del repositorio es extremadamente sencilla y el dise√±o de las clases se adhiere al single-responsibility principle. El directorio modules contiene las clases encargadas de comunicarse con las APIs, mientras que los programas download_tweets.py y extract_features.py se encargan de descargar la informaci√≥n en batches. En el caso del procedimiento de extracci√≥n de tweets, el m√≥dulo twitter_request.py contiene a la clase TwitterRequest, responsable de comunicarse con la Twitter API V2 siguiendo los siguientes pasos:\n\nConsultar el endpoint GET_2_tweets_search_recent de la API utilizando al m√©todo search_recent_tweets para extraer los tweets que necesitamos.\nProcesar la respuesta de la API en formato JSON, convierti√©ndola primero en un DataFrame.\nSeparar a ese DataFrame en dos: Uno que contenga informaci√≥n de los tweets y otro que contenga informaci√≥n de los usuarios que realizaron dichas publicaciones.\nPreprocesar ambos DataFrames para estandarizar los nombres de las columnas y cambiar las columnas que contienen fechas a un formato m√°s conveniente.\n\nEn esta clase, el m√©todo m√°s importante es make_request. Este m√©todo est√° decorado con @backoff.on_exception una forma incre√≠blemente conveniente de hacer exponential backoff en el momento en el que se produzca un error por generar demasiados requests o una excepci√≥n producto de un timeout entre nuestro cliente y el servidor. En cualquiera de estos casos, la funci√≥n esperar√° un tiempo exponencialmente creciente antes de volver a intentarlo y √∫nicamente har√° un m√°ximo de 5 intentos.\n\nimport tweepy\nimport backoff\nimport requests\nfrom modules import setup_logger\n\n@backoff.on_exception(\n        backoff.expo, \n        (tweepy.errors.TooManyRequests, requests.exceptions.ReadTimeout),\n        max_tries=5\n    )\ndef make_request(self) -> \"TwitterRequest\":\n    self.query = f\"{self.query} -is:retweet -is:reply\"\n    self.logger.info(\"Making request with query: %s\", self.query)\n\n    try:\n        self.tweets = client.search_recent_tweets(\n            query = self.query,\n            start_time = self.start_time,\n            end_time = self.end_time,\n            max_results = self.max_results,\n            tweet_fields = [\n                \"id\", \"author_id\", \"created_at\", \"text\", \n                \"public_metrics\", \"possibly_sensitive\", \"lang\"\n            ],\n            user_fields = [\n                \"id\", \"username\", \"name\", \"location\", \"created_at\", \"description\", \n                \"profile_image_url\", \"verified\", \"public_metrics\"\n            ],\n            expansions = [\n                \"author_id\", \"referenced_tweets.id\"\n            ]\n        )\n        \n        if self.tweets is None or self.tweets.data is None:\n            self.logger.error(\"No tweets returned from request\")\n            return self\n          \n    except Exception as e: \n        self.logger.error(\"Exception occurred\", exc_info=True)\n        raise \n\n    self.logger.info(\"Request completed successfully.\")\n    return self\n\nEl mismo m√©todo define el query que se llevar√° a cabo, que en nuestro caso es un string con el nombre del candidato mencionado en los tweets, pero instruimos a la API para que no devuelva tweets que sean retweets o respuestas (f\"{self.query} -is:retweet -is:reply\"). En la solicitud, se incluyen campos espec√≠ficos de los tweets y del usuario que los public√≥. Por ejemplo, del tweet se solicita el ID, el autor, la fecha de creaci√≥n, el texto, las m√©tricas como cantidad de retweets, likes o respuestas, as√≠ como si la publicaci√≥n tienen contenido sensible y cu√°l es su idioma. Del usuario solicitamos el ID, el nombre de usuario, el nombre, la ubicaci√≥n, la fecha de creaci√≥n del perfil, la descripci√≥n, la imagen de perfil, sus m√©tricas p√∫blicas y si su perfil est√° verificado o no2.2¬†Aunque mis procedimientos extrajeron toda esta informaci√≥n para m√°s de 5,000 tweets que fueron publicados por m√∫ltiples usuarios durante las √∫ltimas dos semanas, el Developer Agreement and Policy de Twitter me prohibe publicar la totalidad de la informaci√≥n. Sin embargo, he publicado un dataset reducido que contiene 1,420 tweets con sus respectivas m√©tricas p√∫blicas y las caracter√≠sticas extra√≠das utilizando al modelo de lenguaje.\nSi la solicitud es exitosa, se obtiene cierta cantidad de tweets que son almacenados en el atributo self.tweets. Si no se obtiene ning√∫n tweet, se registra un error en el log. Si ocurre alguna otra excepci√≥n durante la solicitud, tambi√©n se registra en el log y es levantada para que pueda ser manejada por el decorador encargado de hacer exponential backoff. Por √∫ltimo, si la solicitud se complet√≥ con √©xito, registramos este hecho en el log y retornamos al objeto self, para darnos la posibilidad de hacer method chaining.\nDe esta forma, utilizar la clase TwitterRequest requiere √∫nicamente de inicializarla con los par√°metros query, start_time, end_time y max_results. Like so:\n\nfrom datetime import datetime\nfrom modules import TwitterRequest\n\ntweets, users = (\n  TwitterRequest(\n      query='zury rios',\n      start_time=datetime(2023, 5, 20, 15, 00),\n      end_time=datetime(2023, 5, 21, 15, 00),\n      max_results=10\n  )\n  .make_request()\n  .tweets_to_dataframe()\n  .users_to_dataframe()\n  .segregate_dataframe()\n  .preprocess_data(\n      tweets_prefix='tw_',\n      users_prefix='us_'\n  )\n)\n\n\n\nDescargando tweets en batches\nTenemos una forma de comunicarnos con la API de Twitter, pedirle los datos que nos interesan y preprocesarlos para que est√©n listos para el resto del pipeline. Sin embargo, Twitter no nos va a hacer la vida f√°cil. Su API √∫nicamente permite descargar tweets publicados en los √∫ltimos 7 d√≠as en batches de 60 tweets como m√°ximo en un espacio de 15 minutos.33¬†Overprotective much?\nNi modo. El programa download_tweets.py esta dise√±ado para lidiar con las restricciones de la API de Twitter. El m√©todo download_tweets es un componente de la clase DownloadTweets. En resumen, este m√©todo descarga tweets y usuarios (utilizando el m√©todo get_batch, que es un wrapper para la clase TwitterRequest). Para cada candidato y rango de fechas, se invoca el m√©todo get_batch para obtener los tweets y usuarios, que se recopilan y se concatenan en dos atributos: self.tweets y self.users. Finalmente, el m√©todo devuelve estos dos DataFrames que contienen todos los tweets y usuarios recolectados.\n\nimport pandas as pd\nfrom typing import Tuple\nfrom modules import setup_logger\nfrom modules import TwitterRequest\n\ndef download_tweets(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    self.generate_dates()\n    logger.info(f\"Generated {len(self.dates)} date pairs for tweet downloads.\")\n\n    # Collect tweets and users for each candidate.\n    tweets_collector, users_collector = [], []\n    for candidate in self.candidates:\n\n        # Collect tweets and users for each date.\n        dates_tweets_collector, dates_users_collector = [], []\n        for start_date, end_date in self.dates:\n\n            tweets, users = self.get_batch(candidate, start_date, end_date)\n            dates_tweets_collector.append(tweets)\n            dates_users_collector.append(users)\n\n        tweets_collector.append(pd.concat(dates_tweets_collector))\n        users_collector.append(pd.concat(dates_users_collector))\n\n    self.tweets = pd.concat(tweets_collector, axis=0, ignore_index=True)\n    self.users = pd.concat(users_collector, axis=0, ignore_index=True)\n\n    logger.info(\n      f\"Downloaded a total of {len(self.tweets)} tweets and {len(self.users)} users.\"\n    )\n    return self.tweets, self.users"
  },
  {
    "objectID": "posts/election-insights/index.html#zero-shot-feature-extraccion-con-gpt-3.5",
    "href": "posts/election-insights/index.html#zero-shot-feature-extraccion-con-gpt-3.5",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "3 Zero-shot feature extraccion con GPT-3.5",
    "text": "3 Zero-shot feature extraccion con GPT-3.5\nAhora viene una de mis partes favoritas en este proceso. Vamos a usar a GPT-3.5 para generar nuevas features basadas en los tweets que hemos extra√≠do. ¬øC√≥mo? Much in the same way we downloaded tweets, usando una clase llamada OpenAIRequest (que se encargar√° de comunicarse con la API) y otra, llamada FeatureExtraction, que nos servir√° para iterar en las filas del DataFrame que contiene las publicaciones de los usuarios de Twitter.\n\nEnviando requests hacia la API\nEl primer m√©todo relevante en la clase OpenAIRequest es make_request. De manera similar al m√©todo con el mismo nombre en el procedimiento de extracci√≥n de tweets, esta funci√≥n est√°tica est√° decorada con backoff.on_exception para poder lidiar con errores en el caso de un timeout o por sobrepasar los l√≠mites de uso de la API. En el caso del modelo gpt-3.5-turbo, podemos realizar una cantidad de 3,500 requests por minuto o enviar al endpoint un m√°ximo 90,000 tokens en la misma cantidad de tiempo, lo que pase primero.\nComo veremos m√°s adelante, el prompt con el que instruiremos al modelo tiene, en promedio, 600 tokens.4 En teor√≠a, esto quiere decir que (bas√°ndonos en el l√≠mite de TPM) podemos enviar 150 requests cada 60 segundos. En la pr√°ctica, though, los tiempos de inferencia de los modelos de lenguaje son bastante altos. En nuestro caso, el tiempo de procesamiento por cada tweet fue de aproximadamente 15 segundos, as√≠ que extraer caracter√≠sticas para 1,420 tweets fue un proceso que tom√≥ 6 horas, give or take.4¬†OpenAI pone a nuestra disposici√≥n este tokenizer, una herramienta que nos permite hacer un recuento de la cantidad de tokens en nuestros prompts. En lo personal, me sirve much√≠simo para calcular los costos de procesamiento y para asegurarme de que mis prompts sean eficientes en t√©rminos de longitud.\nLos par√°metros que acepta make_request son nuestro prompt, la especificaci√≥n del modelo que queremos utilizar y temperature, un par√°metro del modelo de lenguaje que puede ser un n√∫mero entre 0 y 1. La temperatura controla el grado de aleatoriedad en las respuestas del modelo. Un valor de temperature m√°s alto (cerca de 1) hace que el modelo genere respuestas m√°s diversas y creativas, mientras que un valor m√°s bajo (cerca de 0) hace que las respuestas sean m√°s determin√≠sticas o consistentes.55¬†Los transformers, como GPT, son en s√≠ mismos arquitecturas de redes neuronales determin√≠sticas (es decir, generan la misma salida para una entrada espec√≠fica). Sin embargo, se introduce aleatoriedad durante la generaci√≥n de texto a trav√©s del muestreo de diferentes secuencias de palabras de acuerdo a las probabilidades de salida del modelo. En general, cuando usamos al modelo como un paso de procesamiento dentro de nuestros pipelines, queremos que cada respuesta sea lo m√°s consistente posible para reducir las posibilidades de introducir bugs en nuestro sistema.\n\nimport openai\nimport backoff\nimport requests\n\n@staticmethod\n@backoff.on_exception(\n    backoff.expo, \n    (openai.error.RateLimitError, requests.exceptions.ReadTimeout),\n    max_tries=5\n)\ndef make_request(prompt: str, model: str = \"gpt-3.5-turbo\", temperature: float = 0) -> str: \n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=temperature, \n    )\n    return response.choices[0].message[\"content\"]\n\nEl siguiente m√©todo relevante en la clase OpenAIRequest es extract_features. En este m√©todo, definimos el prompt que ser√° utilizado para instruir al modelo de lenguaje. Este prompt es parametrizado a trav√©s de una f string, para permitirnos cambiar f√°cilmente la entrada seg√∫n sea necesario.\nUna vez definido el prompt, realizamos la consulta a la API de OpenAI utilizando el m√©todo make_request. Si la respuesta obtenida es nula o inv√°lida, registramos un error en el log y retornamos None. Adem√°s, para robustecer al procedimiento y manejar posibles excepciones durante la solicitud a la API, hemos envuelto este segmento de c√≥digo dentro de un bloque Try Except. Si todo sale seg√∫n lo planeado y obtenemos una respuesta v√°lida, la misma es cargada como un diccionario JSON y finalmente retornada por el m√©todo extract_features.\n\nimport json\nfrom modules import setup_logger\n\ndef extract_features(self, prefix: str) -> dict:\n    prompt = f\"\"\" (...) \"\"\"\n    \n    try:\n        response = OpenAIRequest.make_request(prompt)\n        if response is None:\n            self.logger.error(\"Received invalid response from OpenAI\")\n            return None\n        response = json.loads(response)\n    except Exception as e:\n        self.logger.error(f\"Exception during API request: {e}\")\n        return None\n\n    return response\n\n\n\nPrompt engineering\nTengo que confesar algo. Hace unas semanas le√≠ en una publicaci√≥n de LinkedIn una frase que dec√≠a algo as√≠ como: ‚ÄúCalling ‚Äòprompt engineering‚Äô the action of using ChatGPT today is like calling ‚Äòsearch engineering‚Äô to googling something in the early 2000s‚Äù. Y la confesi√≥n es que‚Ä¶ pienso que es verdad.66¬†Por cierto, ya que estamos confesando cosas. La idea de esta entrada surgi√≥ porque hace poco m√°s de un mes vi los videos del curso de deeplearning.ai llamado ChatGPT Prompt Engineering for Developers. Es un curso extremadamente corto que tiene informaci√≥n √∫til acerca de los casos de uso del modelo desde una perspectiva program√°tica. Si est√°s leyendo este blog, chances are you‚Äôre also gonna like this.\nNontheless, considero que cuando vamos un paso m√°s all√° y utilizamos estos modelos desde los endpoints que OpenAI pone a nuestra disposici√≥n, el hype que existe hacia el t√©rmino est√° un poco m√°s justificado. Principalmente porque, aunque redactamos la instrucci√≥n en lenguaje natural, el procedimiento recuerda mucho a definir una serie de pasos en cualquier lenguaje de programaci√≥n.\nPara nuestros fines, este prompt dio buenos resultados:\n\n\n\n\n\n\n\n\nprompt = f\"\"\"\nEl siguiente es un tweet que menciona a un candidato presidencial dentro de la contienda electoral 2023 en Guatemala. \n\nPor favor, clasif√≠calo de acuerdo a las siguientes categor√≠as:\n\nValencia (sentimiento general): [positivo, negativo, neutro, otro]\nEmoci√≥n (emoci√≥n principal expresada): [felicidad, tristeza, enojo, miedo, sorpresa, disgusto, otro]\nPostura (actitud hacia el tema): [aprobaci√≥n, desaprobaci√≥n, esperanza, desilusi√≥n, indiferencia, confianza, desconfianza, otro]\nTono (forma de expresarse): [agresivo, pasivo, asertivo, esc√©ptico, ir√≥nico, humor√≠stico, informativo, serio, inspirador, otro]\n\nAdem√°s, eval√∫alo utilizando una escala continua con rango de 0 a 1 en las siguientes dimensiones:\n\nAmabilidad (nivel de cortes√≠a): [0.0 - 1.0]\nLegibilidad (facilidad de lectura): [0.0 - 1.0]\nControversialidad (potencial para generar desacuerdo): [0.0 - 1.0]\nInformatividad (cantidad de informaci√≥n relevante y fundamentada): [0.0 - 1.0]\n\nFormatea tu respuesta como un diccionario de Python con las siguientes llaves:\n\n[valencia, emocion, postura, tono, amabilidad, legibilidad, controversialidad, informatividad]\n\nTweet: '''{self.tweet}'''\n\"\"\"\n\n\n\n\n\n\n\nZero-shot feature extraction\nEstamos muy cerca de obtener los resultados que buscamos. Hasta el momento, los m√≥dulos que hemos implementado nos permiten enviar consultas hacia las APIs de Twitter y OpenAI, as√≠ como descargar lotes de tweets que mencionan a los cinco candidatos que encabezaban la encuesta de Prensa Libre, publicada a inicios de mayo. Solo nos falta una forma procesar estos tweets para extraer variables que nos permitan analizarlos a una mayor profundidad. Enter extract_features.py, un programa en el que la clase OpenAIRequest es instanciada dentro de la clase FeatureExtraction.\nEn el constructor de la clase se inicializan las rutas a dos archivos: df_path que es la ruta al archivo CSV de entrada que contiene los tweets, y results_df_path que es la ruta al archivo csv de salida donde se almacenar√°n las variables extra√≠das.\nEl principal m√©todo de esta clase es extract_features. Este m√©todo primero carga los tweets del archivo CSV de entrada y elimina los duplicados. Luego intenta cargar las caracter√≠sticas ya extra√≠das del archivo CSV de salida. Si este archivo no existe, se inicializa un nuevo DataFrame vac√≠o. El m√©todo luego determina qu√© tweets a√∫n no han sido procesados comparando los tweets en los dos DataFrames y seleccionando aquellos que solo est√°n en el DataFrame de entrada.\n\nimport pandas as pd\nfrom modules import OpenAIRequest\n\ndef extract_features(self):\n    df = pd.read_csv(self.df_path)\n    df = df.drop_duplicates(subset=['tw_texto'], keep='first')\n\n    try:\n        df_results = pd.read_csv(self.results_df_path)\n        df_results = df_results.drop_duplicates(subset=['tw_texto'], keep='first')\n    except FileNotFoundError:\n        df_results = pd.DataFrame()\n\n    df_to_process = df[~df['tw_texto'].isin(df_results['tw_texto'])]\n    df_to_process = df_to_process.dropna()\n\n    for index, row in df_to_process.iterrows():\n        tweet = row['tw_texto']\n        response = (\n            OpenAIRequest(tweet)\n            .preprocess_text()\n            .extract_features(prefix='tw_')\n        )\n        df_result = pd.DataFrame([response], index=[index])\n        df_results = pd.concat([df_results, df_result])\n        df_results.to_csv(self.results_df_path, index=False)\n\nA continuaci√≥n, se procesa cada tweet. Para cada uno, se realiza una solicitud a la API de OpenAI para realizar el procedimiento de feature extraction utilizando al modelo gpt-3.5-turbo. Las caracter√≠sticas extra√≠das se a√±aden al DataFrame de resultados junto con el tweet original y un tag para identificar a qu√© candidato se refiere cada publicaci√≥n.\nFinalmente, despu√©s de procesar todos los tweets, el DataFrame de resultados se guarda en el archivo CSV de salida. Esto se hace despu√©s de procesar cada tweet para evitar la p√©rdida de informaci√≥n en caso de que se produzca un error durante el procedimiento."
  },
  {
    "objectID": "posts/election-insights/index.html#an√°lisis-de-la-opini√≥n-p√∫blica",
    "href": "posts/election-insights/index.html#an√°lisis-de-la-opini√≥n-p√∫blica",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "4 An√°lisis de la opini√≥n p√∫blica",
    "text": "4 An√°lisis de la opini√≥n p√∫blica\nCool. Todos los procedimientos anteriores nos llevan a esta situaci√≥n en la que tenemos un conjunto de datos listo para ser analizado. Como mencion√© en alg√∫n punto de esta entrada, he publicado este dataset en un repositorio de GitHub. S√≠entete libre de descargarlo y hacer tu propio an√°lisis exploratorio de datos; si lo haces, me encantar√≠a conocer qu√© encuentras. Sin nada m√°s que agregar, el dataset luce as√≠:\n\n\n\n\n \n  \n    fecha \n    tweet \n    candidato \n    retweets \n    replies \n    likes \n    quotes \n    impresiones \n    valencia \n    emocion \n    postura \n    tono \n    amabilidad \n    legibilidad \n    controversialidad \n    informatividad \n    sensitivo \n  \n \n\n  \n    2023-05-15 \n    Sandra Torres es capaz de todo por mantener el poder. Recibi√≥ dinero sucio, se vendi√≥ con Alejandro Giammattei para evitar que cancelaran su partido, incluy√≥ a se√±alados de corrupci√≥n y particip√≥ de una red de captaci√≥n de fondos il√≠citos. https://t.co/4J2pX1BSx6 \n    sandra torres \n    0 \n    1 \n    0 \n    0 \n    335 \n    negativo \n    enojo \n    desaprobaci√≥n \n    agresivo \n    0.2 \n    0.8 \n    0.9 \n    0.9 \n    FALSE \n  \n  \n    2023-05-15 \n    Sandra Torres anda regalando zapatos a cambio de votos üò° esto tiene que parar https://t.co/8OltsAmeiq \n    sandra torres \n    0 \n    0 \n    5 \n    0 \n    420 \n    negativo \n    enojo \n    desaprobaci√≥n \n    agresivo \n    0.2 \n    0.9 \n    0.8 \n    0.8 \n    FALSE \n  \n  \n    2023-05-15 \n    Y en donde est√°n los ladrones como Sandra torres. Baldizon. Los arzu. Portillo. Los r√≠os montt. Los jimmy morales. Los Giammattei y tantos diputados. Que le han robado millones  al pueblo de guatemala. https://t.co/KVngH40GF9 \n    sandra torres \n    0 \n    0 \n    0 \n    0 \n    234 \n    negativo \n    enojo \n    desaprobaci√≥n \n    agresivo \n    0.2 \n    0.8 \n    0.7 \n    0.9 \n    FALSE \n  \n  \n    2023-05-15 \n    #AlertaPopulista üö®\n\n¬øSandra Torres, sabe cu√°les son derechos humanos?\n\nTe presentamos el episodio n√∫mero 5 de nuestra secci√≥n \"El Populista de la Semana\" con @PalmieriWaelti\n\n#EleccionesGuatemala #Elecciones2023 #Guatemala #EleccionesGT #Populistas #Facts #Noticias https://t.co/MeoijdOE12 \n    sandra torres \n    6 \n    9 \n    17 \n    1 \n    3186 \n    negativo \n    enojo \n    desaprobaci√≥n \n    agresivo \n    0.2 \n    0.9 \n    0.8 \n    0.8 \n    FALSE \n  \n  \n    2023-05-15 \n    La CC dej√≥ en firme la inscripci√≥n del binomio presidencial de la UNE conformado por Sandra Torres y Romeo Estuardo Guerra Lemus. üîΩ https://t.co/yG7AEiXV1Y \n    sandra torres \n    0 \n    0 \n    0 \n    0 \n    129 \n    neutro \n    otro \n    informaci√≥n \n    informativo \n    0.8 \n    1.0 \n    0.2 \n    1.0 \n    FALSE \n  \n  \n    2023-05-15 \n    ü§îü§£\n¬øLa hija de Sandra Torres hablando de izquierdosos?\n¬øWTF?!!! https://t.co/Jd4ZaiQ9K3 https://t.co/qk4vZo50aD \n    sandra torres \n    0 \n    0 \n    2 \n    0 \n    180 \n    negativo \n    enojo \n    desaprobaci√≥n \n    ir√≥nico \n    0.2 \n    1.0 \n    0.8 \n    0.6 \n    TRUE \n  \n\n\n\n\n\n\nExplorando el dataset\nComo podemos ver, la informaci√≥n extra√≠da considera b√°sicamente las √∫ltimas dos semanas del mes de marzo.\n\n\nC√≥digo\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\nfechas <- \n  df |> \n  summarise(\n    `Fecha inicial` = min(fecha),\n    `Fecha final` = max(fecha)\n  )\n\nkable(fechas, align = \"c\") |> \n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), \n    full_width = TRUE,\n    font_size = 13\n  )\n\n\n\n\n \n  \n    Fecha inicial \n    Fecha final \n  \n \n\n  \n    2023-05-15 \n    2023-05-27 \n  \n\n\n\n\n\nDurante los procedimientos de extracci√≥n se defini√≥ la misma cantidad de tweets para cada candidato. Sin embargo, al eliminar tweets duplicados podemos notar que, durante el periodo de tiempo del an√°lisis, las personas en Twitter hablaron casi cuatro veces m√°s de Carlos Pineda (quien, para el momento de la extracci√≥n de tweets, a√∫n segu√≠a participando en la contienda electoral) que de Manuel Conde.\n\n\nC√≥digo\ndf |> \n  group_by(candidato) |> \n  summarise(\n    count = n()\n  ) |> \n  ggplot(aes(reorder(candidato, -count), count, fill = candidato)) +\n  geom_col(alpha = 0.6) +\n  labs(\n    title = \"Cantidad de tweets por candidato\"\n  ) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_manual(values = palette)\n\n\n\n\n\n\n\n\n\n\n \n  \n    Candidato \n    Engagement \n  \n \n\n  \n    carlos pineda \n    0.8963113 \n  \n  \n    edmond mulet \n    1.4452980 \n  \n  \n    manuel conde \n    1.6900648 \n  \n  \n    sandra torres \n    1.4844873 \n  \n  \n    zury rios \n    1.9128208 \n  \n\n\n\n\nPero no solo podemos fijarnos en la cantidad de tweets que mencionan a los candidatos, ser√≠a importante tambi√©n considerar la calidad de estas publicaciones. Para ello, vamos a computar el engagement rate de cada publicaci√≥n, definido como la suma de interacciones (retweets, replies, likes y quotes) de cada tweet, dividida entre la cantidad de impresiones que la publicaci√≥n tuvo. Curiosamente, los tweets que mencionan a Carlos Pineda son los que tienen un menor engagement. Mmm, muchos tweets con poco engagement‚Ä¶ ¬øa ustedes tambi√©n les suena raro?\nLa siguiente gr√°fica muestra la distribuci√≥n del logaritmo del engagement rate por cada uno de los candidatos. Parece que hablar de Zury R√≠os es una buena forma de conseguir atenci√≥n. Aunque no por mucho. En promedio, solo el 1.9% de las impresiones de los tweets que hablaban de Zury R√≠os resultaron en alg√∫n tipo de interacci√≥n.\n\n\nC√≥digo\ndf |> \n  mutate(\n    engagement = log(((retweets + replies + likes + quotes) / impresiones * 100))\n  ) |> \n  group_by(candidato) |> \n  ggplot(aes(engagement, candidato, fill = candidato)) +\n  geom_boxplot(alpha = 0.6, ) +\n  labs(title = \"Logaritmo de la tasa de engagement por cada candidato\") +\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_manual(values = palette) \n\n\n\n\n\n\n\nAnalizando sentimientos\nObviamente, lo m√°s interesante de este an√°lisis es utilizar las variables que fueron generadas por gpt-3.5-turbo. Podemos empezar por preguntarnos si existe alguna relaci√≥n entre la cantidad de interacciones y la valencia (el sentimiento general) del tweet. Y my oh my, resulta que s√≠.\n\n\nOne-way ANOVA\n\n\n\n\n \n  \n    term \n    statistic \n    p.value \n  \n \n\n  \n    retweets \n    9.241632 \n    0.0001029 \n  \n  \n    replies \n    2.783355 \n    0.0621688 \n  \n\n\n\n\n\nTweets con sentimiento negativo tienen m√°s retweets en promedio, mientras que publicaciones con sentimiento positivo tienen, en promedio, m√°s cantidad de respuestas. De hecho, la diferencia entre las medias del logaritmo de la cantidad de retweets segregados por la valencia es estad√≠sticamente significativa, como lo muestra la tabla al margen de esta secci√≥n, en la que he realizado un an√°lisis de varianza, tanto para los tweets como para las respuestas.77¬†Sin embargo, no es posible rechazar la hip√≥tesis nula para el caso de las respuestas. Pero esto a todas luces es debido a que las medias del logaritmo de la cantidad de respuestas para tweets con sentimiento negativo y neutro son pr√°cticamente iguales.\n\n\nC√≥digo\nlibrary(patchwork)\n\nretweets <- \n  df |> \n  group_by(candidato) |> \n  ggplot(aes(valencia, log(retweets), fill = valencia)) +\n  geom_boxplot(alpha = 0.6) +\n  labs(y = \"Logaritmo de la cantidad de retweets\") +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    legend.title = element_blank(),\n    legend.position = \"none\"\n  ) +\n  scale_fill_manual(values = palette) \n\nreplies <- \n  df |> \n  group_by(candidato) |> \n  ggplot(aes(valencia, log(replies), fill = valencia)) +\n  geom_boxplot(alpha = 0.6) +\n  labs(y = \"Logaritmo de la cantidad de replies\") +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    legend.title = element_blank(),\n    legend.position = \"none\"\n  ) +\n  scale_fill_manual(values = palette) \n\nretweets + replies + \n  plot_annotation(\"Relaci√≥n entre los tipos de interacci√≥n y la valencia del mensaje\")\n\n\n\n\n\nOtra pregunta importante es c√≥mo se relacionan la controversialidad de los tweets‚Äîdefinida aqu√≠ como el potencial de una publicaci√≥n para generar desacuerdo‚Äîy la cantidad de veces que son retuiteados. Podr√≠amos esperar que a mayor controversialidad, mayor sea la probabilidad de obtener m√°s retweets. Esta hip√≥tesis parece confirmarse al observar las regresiones polinomiales locales ajustadas para cada candidato, que demuestran una relaci√≥n positiva entre la controversialidad y la cantidad de retweets.\nEn estas regresiones, hemos incluido la puntuaci√≥n de controversialidad de los tweets, calculada utilizando el modelo GPT durante la extracci√≥n de caracter√≠sticas. En general, los resultados sugieren que los tweets m√°s controvertidos tienden a ser retuiteados con mayor frecuencia para cada uno de los cinco candidatos en nuestro an√°lisis, en promedio.\n\n\nC√≥digo\ndf |> \n  ggplot(aes(controversialidad, log(retweets), color = candidato)) + \n  geom_jitter(height = 0.3, alpha = 0.6) + \n  geom_smooth(se = FALSE, alpha = 0.6) +\n  labs(\n    title = \"Relaci√≥n entre la controversialidad y el logaritmo de la cantidad de retweets\",\n    x = \"Controversialidad del tweet\",\n    y = \"Logaritmo de la cantidad de retweets\"\n  ) +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  scale_color_manual(values = palette) \n\n\n\n\n\nPara finalizar nuestro an√°lisis, representamos gr√°ficamente la distribuci√≥n de los tweets divididos en cuatro categor√≠as emocionales: aprobaci√≥n, desaprobaci√≥n, esperanza e indiferencia. De forma general, se observa una predominancia de la desaprobaci√≥n en todos los candidatos analizados, con un promedio del 66 por ciento.\nSandra Torres encabeza esta tendencia, casi un 80 por ciento de los tweets en los que se le menciona reflejan desaprobaci√≥n. Al mismo tiempo, Torres presenta la segunda menor proporci√≥n de tweets con tono de aprobaci√≥n, siendo solo superada por Carlos Pineda. En cuanto a la indiferencia, Manuel Conde es el candidato que acumula la mayor cantidad de tweets bajo esta postura, en comparaci√≥n con los dem√°s contendientes.\n\n\nC√≥digo\ndf |> \n  group_by(candidato, postura) |> \n  summarise(cantidad = n()) |> \n  filter(str_detect(postura, \"aprobaci√≥n|desaprobaci√≥n|esperanza|indiferencia\")) |> \n  mutate(cantidad = cantidad / sum(cantidad)) |> \n  ggplot(aes(reorder(candidato, cantidad), cantidad, fill = postura)) +\n  geom_col(position = \"dodge\", alpha = 0.6) +\n  labs(title = \"Proporci√≥n de tweets por candidato seg√∫n la postura del usuario\") +\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_manual(values = palette)"
  },
  {
    "objectID": "posts/election-insights/index.html#conclusiones",
    "href": "posts/election-insights/index.html#conclusiones",
    "title": "Revelando los sentimientos guatemaltecos",
    "section": "5 Conclusiones",
    "text": "5 Conclusiones\nDurante esta entrada, utilizamos la API de Twitter y la API de OpenAI para decodificar los sentimientos de los usuarios de Twitter en torno a cinco candidatos presidenciales en la contienda electoral de 2023 en Guatemala. Este estudio demostr√≥ la eficacia de las tecnolog√≠as avanzadas en la comprensi√≥n de la din√°mica social y pol√≠tica, ilustrando c√≥mo foundation models como gpt-3.5-turbo pueden emplearse para extraer caracter√≠sticas valiosas y generar insights significativos.\nDescargamos tweets que mencionan a los candidatos mediante la Twitter API V2 y los utilizamos como nuestro conjunto de datos base. Luego, extra√≠mos caracter√≠sticas de los tweets utilizando la API de OpenAI y realizamos una exploraci√≥n inicial de los resultados con R.\nMis reflexiones personales tras realizar estos experimentos se resumen en los siguientes puntos:\n\nPrimero, al interactuar con APIs, la robustez es clave. La capacidad de gestionar eficazmente errores y excepciones es fundamental. Un estudio de Papasian & Underwood (2020), dos ingenieros de aprendizaje autom√°tico en Google, destaca que la mayor√≠a de los problemas en sistemas de machine learning no se deben a la implementaci√≥n de los modelos per se, sino a fallas en los flujos de trabajo de los data pipelines.\nEn segundo lugar, mis experiencias con la API de OpenAI han revelado que, si bien GPT es bastante competente en la mayor√≠a de las tareas que he probado, tiene sus limitaciones. Una desventaja notable es el fen√≥meno popularmente conocido como ‚Äúalucinaciones‚Äù. En edge cases (situaciones inusuales dentro del contenido de los prompts que se env√≠an al modelo), este tiende a inventar nuevas categor√≠as que pueden parecer m√°s adecuadas para el texto, pero que introducen etiquetas que, en teor√≠a, no deber√≠an existir en el conjunto de datos.\nPor √∫ltimo, hay que tener en cuenta que los tiempos de inferencia son relativamente largos. Esto plantea desaf√≠os de escalabilidad al usar este modelo como m√©todo de procesamiento, especialmente a medida que el conjunto de datos se expande.\n\nEste estudio evidencia el emocionante cruce entre las ciencias de la computaci√≥n y las ciencias sociales, y c√≥mo esta intersecci√≥n permite generar insights de eventos significativos a trav√©s de tecnolog√≠as avanzadas. Pese a los desaf√≠os, el resultado es un conjunto de datos valiosos y un an√°lisis significativo de las opiniones de los votantes sobre los candidatos presidenciales."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Gabriel Fuentes",
    "section": "",
    "text": "Apasionado por tareas de predicci√≥n de series de tiempo macroecon√≥micas, inferencia microeconom√©trica orientada al an√°lisis de la conducta humana y programaci√≥n estad√≠stica con Python, R y SQL.\n\nEducaci√≥n\nUniversidad Rafael Land√≠var | Licenciatura  Econom√≠a Empresarial | 2017 - 2023"
  }
]